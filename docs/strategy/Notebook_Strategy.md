# **Notebook Implementation Strategy**

## **1. Guiding Philosophy: Notebooks as Learning & Analysis Tools**

This strategy positions Jupyter notebooks as the primary tool for the **"teaching" and "analysis"** phases of the project, not for running the primary training experiments themselves. The core idea is to separate the execution of experiments from their narrative explanation and analysis.

* **Command Line for Execution:** All training and evaluation runs are executed via the standardized train.py and evaluate.py scripts. This ensures reproducibility and consistency.  
* **Notebooks for Analysis:** Notebooks will consume the *outputs* of these command-line runs (logs, saved models, and visualization images) to build a rich, step-by-step analysis.

## **2. The Three-Notebook Model**

For each Keystone and Side-quest model, we will implement a set of three distinct notebooks within its /notebooks/ directory. This structure mirrors the learning phases defined in your "Learning & Development Strategy."

### **Notebook 1: 01_Theory_and_Intuition.ipynb**

* **Purpose:** To serve as the interactive version of the "Theoretical Deep Dive." This notebook is for building a deep, conceptual understanding of the model *before* looking at the full implementation.  
* **Contents:**  
  * **Historical Context:** The "5 Ws" (Who, What, When, Where, Why) with markdown, images, and links to the original papers.  
  * **Architectural Blueprint:** High-level diagrams and explanations of the model's structure.  
  * **Mathematical Intuition:** Use LaTeX to render key equations, but more importantly, use simple, illustrative Python/NumPy code snippets to build intuition for what the math is doing (e.g., demonstrating a single convolution on a tiny matrix). These snippets are for teaching and are separate from the main project code.

### **Notebook 2: 02_Code_Walkthrough.ipynb**

* **Purpose:** To provide a guided tour of your finished code, explaining the "how" and connecting it back to the theory. This notebook does **not** replicate the code from /src; it **imports and explains it**.  
* **Contents:**  
  * **Setup & Configuration:** Import modules from /src and load the configuration from config.py, explaining each parameter.  
  * **Model Architecture:** Instantiate the model from model.py, print its summary (print(model)), and walk through each layer, explaining its purpose.  
  * **A Single Step:** Demonstrate a single forward and backward pass using a single batch of data to illustrate the training mechanics without running the full loop. This is an invaluable pedagogical tool.

### **Notebook 3: 03_Empirical_Analysis.ipynb**

* **Purpose:** To create the data-driven narrative for the "Demonstrate & Expose" phase. This notebook analyzes the results of a completed training run.  
* **Contents:**  
  * **Loading Results:** This notebook begins by loading the artifacts generated by a train.py run: the log file from /outputs/logs/ and the saved image files from /outputs/visualizations/.  
  * **The "Success" & "Failure" Cases:** This is where you will build your analysis. You will load the saved visualizations one by one in separate cells, each accompanied by a markdown cell explaining what the plot shows and what conclusions can be drawn from it.  
  * **The Transition Narrative:** Conclude by summarizing the model's limitations as demonstrated by the analysis, creating a clear motivation for the next model in the series.

---

## **3. Detailed Notebook Templates**

### **Template 1: Theory and Intuition Notebook**

#### **Expected Structure and Sections**

**Cell 1: Title and Introduction**

```markdown
# [Model Name]: Theory and Intuition

## Overview
Brief description of the model's purpose and key innovation.

## Learning Objectives
By the end of this notebook, you will understand:
- The historical context that motivated this model
- The key mathematical concepts and intuitions
- How this model differs from previous approaches
- The theoretical strengths and limitations

## Prerequisites
- Basic understanding of [prerequisite models/concepts]
- Familiarity with [mathematical concepts]
```

**Cell 2: Historical Context**

```markdown
# 1. Historical Context

## The Problem
Describe the specific problem this model was designed to solve.

## Timeline
- **Year**: [Year introduced]
- **Authors**: [Author names]
- **Paper**: "[Paper title]" ([Paper link])
- **Context**: What was happening in AI/ML at the time?

## The Innovation
What was the key breakthrough or innovation?
```

**Cell 3: Mathematical Foundation**

```python
# 2. Mathematical Foundation

import numpy as np
import matplotlib.pyplot as plt
from IPython.display import display, Markdown, Math

# Example: Demonstrate core mathematical concept
def demonstrate_key_concept():
    """
    Interactive demonstration of the core mathematical concept.
    
    This should be simple, visual, and educational.
    """
    # Simple example with small data
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y = np.array([0, 1, 1, 0])  # XOR problem
    
    print("XOR Problem - why linear models fail:")
    print("Input X:", X)
    print("Target y:", y)
    
    # Visualize the problem
    plt.figure(figsize=(8, 6))
    colors = ['red' if label == 0 else 'blue' for label in y]
    plt.scatter(X[:, 0], X[:, 1], c=colors, s=200, alpha=0.7)
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('XOR Problem: Not Linearly Separable')
    plt.grid(True, alpha=0.3)
    plt.show()
    
    return X, y

X, y = demonstrate_key_concept()
```

**Cell 4: Architectural Intuition**

```python
# 3. Architectural Intuition

def visualize_architecture():
    """
    Create a simple visualization of the model architecture.
    
    This should show the flow of information through the model.
    """
    fig, ax = plt.subplots(1, 1, figsize=(12, 8))
    
    # Example: MLP architecture visualization
    layers = [
        {'name': 'Input', 'size': 2, 'x': 0, 'color': 'lightblue'},
        {'name': 'Hidden 1', 'size': 4, 'x': 2, 'color': 'lightgreen'},
        {'name': 'Hidden 2', 'size': 3, 'x': 4, 'color': 'lightgreen'},
        {'name': 'Output', 'size': 1, 'x': 6, 'color': 'lightcoral'}
    ]
    
    for layer in layers:
        y_positions = np.linspace(-layer['size']/2, layer['size']/2, layer['size'])
        for y in y_positions:
            circle = plt.Circle((layer['x'], y), 0.3, color=layer['color'], ec='black')
            ax.add_patch(circle)
        ax.text(layer['x'], -layer['size']/2 - 0.8, layer['name'], ha='center', fontsize=12)
    
    # Draw connections
    for i in range(len(layers) - 1):
        curr_layer = layers[i]
        next_layer = layers[i + 1]
        
        curr_y = np.linspace(-curr_layer['size']/2, curr_layer['size']/2, curr_layer['size'])
        next_y = np.linspace(-next_layer['size']/2, next_layer['size']/2, next_layer['size'])
        
        for cy in curr_y:
            for ny in next_y:
                ax.plot([curr_layer['x'] + 0.3, next_layer['x'] - 0.3], 
                       [cy, ny], 'k-', alpha=0.3, linewidth=0.5)
    
    ax.set_xlim(-0.5, 6.5)
    ax.set_ylim(-3, 3)
    ax.set_aspect('equal')
    ax.axis('off')
    ax.set_title('Model Architecture', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()

visualize_architecture()
```

**Cell 5: Step-by-Step Mathematical Walkthrough**

```python
# 4. Step-by-Step Mathematical Walkthrough

def step_by_step_forward_pass():
    """
    Demonstrate a single forward pass through the model.
    
    This should be pedagogical, showing each step clearly.
    """
    print("Step-by-Step Forward Pass")
    print("=" * 40)
    
    # Simple example with known weights
    input_data = np.array([[1.0, 0.5]])
    W1 = np.array([[0.2, 0.3, -0.1], [0.4, -0.2, 0.6]])  # 2x3
    b1 = np.array([0.1, 0.0, -0.1])
    W2 = np.array([[0.5], [-0.3], [0.4]])  # 3x1
    b2 = np.array([0.2])
    
    print(f"Input: {input_data}")
    print(f"Input shape: {input_data.shape}")
    print()
    
    # Layer 1
    z1 = np.dot(input_data, W1) + b1
    print(f"Layer 1 - Linear transformation:")
    print(f"  z1 = X @ W1 + b1")
    print(f"  z1 = {input_data} @ {W1} + {b1}")
    print(f"  z1 = {z1}")
    print()
    
    # Activation
    a1 = np.maximum(0, z1)  # ReLU
    print(f"Layer 1 - ReLU activation:")
    print(f"  a1 = max(0, z1) = {a1}")
    print()
    
    # Layer 2
    z2 = np.dot(a1, W2) + b2
    print(f"Layer 2 - Linear transformation:")
    print(f"  z2 = a1 @ W2 + b2")
    print(f"  z2 = {a1} @ {W2} + {b2}")
    print(f"  z2 = {z2}")
    print()
    
    # Output
    output = 1 / (1 + np.exp(-z2))  # Sigmoid
    print(f"Output - Sigmoid activation:")
    print(f"  output = 1 / (1 + exp(-z2)) = {output}")
    
    return output

output = step_by_step_forward_pass()
```

**Cell 6: Conceptual Limitations**

```markdown
# 5. Theoretical Limitations

## What This Model Can Do
- List the theoretical capabilities
- Explain why these capabilities exist

## What This Model Cannot Do
- List the fundamental limitations
- Explain why these limitations exist (mathematical/architectural reasons)

## Motivation for Next Model
Based on these limitations, what kind of model would we need next?
```

### **Template 2: Code Walkthrough Notebook**

#### **Expected Structure and Sections**

**Cell 1: Setup and Imports**

```python
# Code Walkthrough: [Model Name]

import sys
import os
sys.path.append('../src')  # Add src to path

# Standard imports
import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from pathlib import Path

# Import our model components
from config import get_config
from model import ModelClass
from constants import *

# Set up environment
torch.manual_seed(42)
np.random.seed(42)

print("Environment setup complete!")
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
```

**Cell 2: Configuration Deep Dive**

```python
# 1. Configuration Deep Dive

# Load configuration for our main experiment
config = get_config('xor')

print("Configuration Analysis")
print("=" * 50)

# Group and explain configuration parameters
config_groups = {
    'Model Architecture': [
        'model_name', 'input_size', 'hidden_size', 'output_size', 
        'num_layers', 'activation', 'dropout'
    ],
    'Training Parameters': [
        'learning_rate', 'batch_size', 'epochs', 'optimizer', 
        'loss_function', 'weight_decay'
    ],
    'Data Parameters': [
        'dataset', 'train_split', 'val_split', 'test_split'
    ],
    'Experiment Settings': [
        'experiment', 'seed', 'device', 'plot_types'
    ]
}

for group_name, param_names in config_groups.items():
    print(f"\n{group_name}:")
    for param in param_names:
        if hasattr(config, param):
            value = getattr(config, param)
            print(f"  {param}: {value}")
    print()

# Explain key configuration choices
print("Key Configuration Decisions:")
print(f"‚Ä¢ Learning Rate ({config.learning_rate}): {'High' if config.learning_rate > 0.01 else 'Low'} - suitable for {'simple' if config.learning_rate > 0.01 else 'complex'} problems")
print(f"‚Ä¢ Hidden Size ({config.hidden_size}): {'Large' if config.hidden_size > 50 else 'Small'} - {'May overfit' if config.hidden_size > 100 else 'Appropriate'} for this dataset")
print(f"‚Ä¢ Activation ({config.activation}): {config.activation.upper()} - {'Non-linear' if config.activation != 'linear' else 'Linear'} activation")
```

**Cell 3: Model Architecture Exploration**

```python
# 2. Model Architecture Exploration

# Create model instance
model = ModelClass(config)

print("Model Architecture Analysis")
print("=" * 50)

# Model summary
print(f"Model: {model.__class__.__name__}")
print(f"Total parameters: {sum(p.numel() for p in model.parameters()):,}")
print(f"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
print()

# Layer by layer breakdown
print("Layer-by-Layer Breakdown:")
for i, (name, module) in enumerate(model.named_modules()):
    if len(list(module.children())) == 0:  # Leaf modules only
        param_count = sum(p.numel() for p in module.parameters())
        print(f"  {i}: {name} -> {module} [{param_count:,} parameters]")
print()

# Visualize model structure
print("Model Structure:")
print(model)
print()

# Test forward pass with dummy data
dummy_input = torch.randn(1, config.input_size)
print(f"Testing forward pass...")
print(f"Input shape: {dummy_input.shape}")

with torch.no_grad():
    output = model(dummy_input)
    print(f"Output shape: {output.shape}")
    print(f"Output value: {output.item():.6f}")
```

**Cell 4: Single Training Step Demonstration**

```python
# 3. Single Training Step Demonstration

print("Single Training Step Walkthrough")
print("=" * 50)

# Create a small batch of training data
if config.dataset == 'xor':
    # XOR data
    X = torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]], dtype=torch.float32)
    y = torch.tensor([[0.], [1.], [1.], [0.]], dtype=torch.float32)
else:
    # Random data for demonstration
    X = torch.randn(4, config.input_size)
    y = torch.randint(0, 2, (4, 1)).float()

print(f"Training batch:")
print(f"  X shape: {X.shape}")
print(f"  y shape: {y.shape}")
print(f"  X values:\n{X}")
print(f"  y values:\n{y}")
print()

# Initialize optimizer and loss
optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)
criterion = nn.BCEWithLogitsLoss()

# Forward pass
print("Forward Pass:")
print("-" * 20)
logits = model(X)
print(f"  Model output (logits): {logits.detach().numpy().flatten()}")

# Convert to probabilities
probs = torch.sigmoid(logits)
print(f"  Probabilities: {probs.detach().numpy().flatten()}")

# Compute loss
loss = criterion(logits, y)
print(f"  Loss: {loss.item():.6f}")
print()

# Backward pass
print("Backward Pass:")
print("-" * 20)
optimizer.zero_grad()
loss.backward()

# Show gradients
print("  Gradients:")
for name, param in model.named_parameters():
    if param.grad is not None:
        grad_norm = param.grad.norm().item()
        print(f"    {name}: grad_norm = {grad_norm:.6f}")

# Update parameters
optimizer.step()
print("  Parameters updated!")
print()

# Test the updated model
print("After Parameter Update:")
print("-" * 30)
with torch.no_grad():
    new_logits = model(X)
    new_probs = torch.sigmoid(new_logits)
    new_loss = criterion(new_logits, y)
    
    print(f"  New probabilities: {new_probs.detach().numpy().flatten()}")
    print(f"  New loss: {new_loss.item():.6f}")
    print(f"  Loss change: {new_loss.item() - loss.item():.6f}")
```

**Cell 5: Feature Visualization**

```python
# 4. Feature Visualization (if applicable)

if hasattr(model, 'get_features'):
    print("Feature Visualization")
    print("=" * 30)
    
    # Get intermediate features
    with torch.no_grad():
        features = model.get_features(X)
    
    # Visualize features layer by layer
    fig, axes = plt.subplots(1, len(features), figsize=(4*len(features), 4))
    if len(features) == 1:
        axes = [axes]
    
    for i, feature in enumerate(features):
        feature_np = feature.numpy()
        
        # Plot feature values
        im = axes[i].imshow(feature_np, cmap='viridis', aspect='auto')
        axes[i].set_title(f'Layer {i+1} Features')
        axes[i].set_xlabel('Feature Dimension')
        axes[i].set_ylabel('Sample')
        plt.colorbar(im, ax=axes[i])
    
    plt.tight_layout()
    plt.show()
    
    # Feature statistics
    print("Feature Statistics:")
    for i, feature in enumerate(features):
        print(f"  Layer {i+1}:")
        print(f"    Shape: {feature.shape}")
        print(f"    Mean: {feature.mean().item():.4f}")
        print(f"    Std: {feature.std().item():.4f}")
        print(f"    Min: {feature.min().item():.4f}")
        print(f"    Max: {feature.max().item():.4f}")
else:
    print("Feature visualization not available for this model")
```

**Cell 6: Code Quality Analysis**

```python
# 5. Code Quality Analysis

print("Code Quality Analysis")
print("=" * 30)

# Check model implementation
print("Model Implementation Checks:")
print(f"‚úì Model inherits from nn.Module: {isinstance(model, nn.Module)}")
print(f"‚úì Has forward method: {hasattr(model, 'forward')}")
print(f"‚úì Parameters require gradients: {all(p.requires_grad for p in model.parameters())}")
print(f"‚úì Model in training mode: {model.training}")
print()

# Check configuration
print("Configuration Checks:")
print(f"‚úì Has experiment name: {hasattr(config, 'experiment') and config.experiment is not None}")
print(f"‚úì Has valid learning rate: {0 < config.learning_rate < 1}")
print(f"‚úì Has valid batch size: {config.batch_size > 0}")
print(f"‚úì Has valid epochs: {config.epochs > 0}")
print()

# Model compatibility
print("Model Compatibility:")
print(f"‚úì Input size matches config: {hasattr(model, 'config') and model.config.input_size == config.input_size}")
print(f"‚úì Output size matches config: {hasattr(model, 'config') and model.config.output_size == config.output_size}")
print(f"‚úì Model can handle batch processing: {True}")  # Already tested above
```

### **Template 3: Empirical Analysis Notebook**

#### **Expected Structure and Sections**

**Cell 1: Setup and Data Loading**

```python
# Empirical Analysis: [Model Name]

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
import re

# Set up plotting style
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# Configuration
MODEL_NAME = "MLP"
EXPERIMENT_NAME = "xor"
OUTPUTS_DIR = Path("../outputs")
LOGS_DIR = OUTPUTS_DIR / "logs"
PLOTS_DIR = OUTPUTS_DIR / "visualizations"
MODELS_DIR = OUTPUTS_DIR / "models"

print(f"Analyzing {MODEL_NAME} results for experiment: {EXPERIMENT_NAME}")
print(f"Looking for files in: {OUTPUTS_DIR}")
print()

# Check if results exist
log_files = list(LOGS_DIR.glob("*.log"))
plot_files = list(PLOTS_DIR.glob("*.png"))
model_files = list(MODELS_DIR.glob("*.pth"))

print(f"Found {len(log_files)} log files")
print(f"Found {len(plot_files)} plot files")
print(f"Found {len(model_files)} model files")
print()

if not log_files:
    print("‚ö†Ô∏è  No log files found. Please run training first:")
    print("   python src/train.py --experiment xor --visualize")
```

**Cell 2: Training Log Analysis**

```python
# 1. Training Log Analysis

def parse_training_log(log_file):
    """Parse training log file and extract metrics."""
    metrics = {
        'epoch': [],
        'train_loss': [],
        'val_loss': [],
        'train_acc': [],
        'val_acc': [],
        'lr': []
    }
    
    with open(log_file, 'r') as f:
        for line in f:
            # Parse different log formats
            if 'Epoch' in line and 'Loss' in line:
                # Extract epoch and loss information
                epoch_match = re.search(r'Epoch (\d+)', line)
                train_loss_match = re.search(r'Train Loss: ([\d.]+)', line)
                val_loss_match = re.search(r'Val Loss: ([\d.]+)', line)
                
                if epoch_match:
                    metrics['epoch'].append(int(epoch_match.group(1)))
                if train_loss_match:
                    metrics['train_loss'].append(float(train_loss_match.group(1)))
                if val_loss_match:
                    metrics['val_loss'].append(float(val_loss_match.group(1)))
    
    return pd.DataFrame(metrics)

# Load and analyze training logs
if log_files:
    log_file = log_files[0]  # Use the most recent log file
    print(f"Analyzing log file: {log_file}")
    
    df = parse_training_log(log_file)
    
    if not df.empty:
        print("\nTraining Summary:")
        print(f"  Total epochs: {len(df)}")
        print(f"  Final train loss: {df['train_loss'].iloc[-1]:.6f}")
        print(f"  Final val loss: {df['val_loss'].iloc[-1]:.6f}")
        print(f"  Best val loss: {df['val_loss'].min():.6f}")
        print(f"  Convergence: {'Yes' if df['val_loss'].iloc[-5:].std() < 0.01 else 'No'}")
        
        # Plot training curves
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))
        
        # Loss curves
        axes[0].plot(df['epoch'], df['train_loss'], label='Training Loss', marker='o')
        axes[0].plot(df['epoch'], df['val_loss'], label='Validation Loss', marker='s')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Loss')
        axes[0].set_title('Training and Validation Loss')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # Loss difference
        loss_diff = df['train_loss'] - df['val_loss']
        axes[1].plot(df['epoch'], loss_diff, label='Train - Val Loss', marker='o', color='red')
        axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.5)
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('Loss Difference')
        axes[1].set_title('Overfitting Analysis')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        # Training insights
        print("\nTraining Insights:")
        if loss_diff.iloc[-1] > 0.1:
            print("  ‚ö†Ô∏è  Model may be overfitting (train loss << val loss)")
        elif loss_diff.iloc[-1] < -0.1:
            print("  ‚ö†Ô∏è  Model may be underfitting (val loss << train loss)")
        else:
            print("  ‚úì Model appears to be well-balanced")
        
        # Convergence analysis
        recent_losses = df['val_loss'].iloc[-10:]
        if recent_losses.std() < 0.01:
            print("  ‚úì Model has converged")
        else:
            print("  ‚ö†Ô∏è  Model may need more epochs to converge")
    else:
        print("No training metrics found in log file")
else:
    print("No log files available for analysis")
```

**Cell 3: Visualization Analysis**

```python
# 2. Visualization Analysis

print("Generated Visualizations Analysis")
print("=" * 40)

# Load and analyze each visualization
def analyze_visualization(plot_file):
    """Analyze a specific visualization file."""
    plot_name = plot_file.stem
    
    print(f"\nAnalyzing: {plot_name}")
    print("-" * 30)
    
    # Display the plot
    from IPython.display import Image, display
    display(Image(str(plot_file)))
    
    # Analysis based on plot type
    if 'loss_curve' in plot_name:
        print("üìä Loss Curve Analysis:")
        print("  ‚Ä¢ Shows training and validation loss over epochs")
        print("  ‚Ä¢ Look for: convergence, overfitting, instability")
        print("  ‚Ä¢ Ideal: smooth decrease, train/val losses close")
        
    elif 'decision_boundary' in plot_name:
        print("üéØ Decision Boundary Analysis:")
        print("  ‚Ä¢ Shows how the model separates different classes")
        print("  ‚Ä¢ Look for: smooth boundaries, correct classification")
        print("  ‚Ä¢ Ideal: clear separation between classes")
        
    elif 'confusion_matrix' in plot_name:
        print("üìà Confusion Matrix Analysis:")
        print("  ‚Ä¢ Shows prediction accuracy for each class")
        print("  ‚Ä¢ Look for: high diagonal values, low off-diagonal")
        print("  ‚Ä¢ Ideal: mostly correct predictions")
        
    elif 'feature_importance' in plot_name:
        print("üîç Feature Importance Analysis:")
        print("  ‚Ä¢ Shows which input features are most important")
        print("  ‚Ä¢ Look for: relevant features have high importance")
        print("  ‚Ä¢ Ideal: matches domain knowledge")
    
    return plot_name

# Analyze all available plots
analyzed_plots = []
for plot_file in sorted(plot_files):
    plot_name = analyze_visualization(plot_file)
    analyzed_plots.append(plot_name)
    print()

if analyzed_plots:
    print(f"‚úì Analyzed {len(analyzed_plots)} visualizations")
    print(f"Plot types: {', '.join(analyzed_plots)}")
else:
    print("No visualization files found")
    print("To generate visualizations, run:")
    print("  python src/train.py --experiment xor --visualize")
```

**Cell 4: Model Performance Evaluation**

```python
# 3. Model Performance Evaluation

print("Model Performance Evaluation")
print("=" * 40)

# Load model if available
if model_files:
    import torch
    import sys
    sys.path.append('../src')
    from model import ModelClass
    from config import get_config
    
    # Load configuration and model
    config = get_config(EXPERIMENT_NAME)
    model = ModelClass(config)
    
    # Load trained weights
    checkpoint = torch.load(model_files[0], map_location='cpu')
    model.load_state_dict(checkpoint)
    model.eval()
    
    print(f"‚úì Loaded model: {model.__class__.__name__}")
    print(f"‚úì Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    print()
    
    # Test on known data
    if EXPERIMENT_NAME == 'xor':
        # XOR test cases
        test_cases = [
            ([0, 0], 0),
            ([0, 1], 1),
            ([1, 0], 1),
            ([1, 1], 0)
        ]
        
        print("XOR Test Results:")
        print("Input  | Target | Predicted | Correct")
        print("-------|--------|-----------|--------")
        
        correct = 0
        total = len(test_cases)
        
        with torch.no_grad():
            for inputs, target in test_cases:
                x = torch.tensor([inputs], dtype=torch.float32)
                output = model(x)
                prediction = torch.sigmoid(output).item()
                predicted_class = 1 if prediction > 0.5 else 0
                is_correct = predicted_class == target
                
                print(f"{inputs} | {target:>6} | {prediction:>9.4f} | {'‚úì' if is_correct else '‚úó'}")
                
                if is_correct:
                    correct += 1
        
        accuracy = correct / total
        print(f"\nAccuracy: {accuracy:.1%} ({correct}/{total})")
        
        if accuracy >= 0.9:
            print("‚úÖ Model successfully learned XOR function!")
        else:
            print("‚ùå Model failed to learn XOR function")
    
    else:
        print(f"Performance evaluation for {EXPERIMENT_NAME} not implemented")
        print("Model loaded successfully and ready for testing")

else:
    print("No model files found")
    print("Train a model first with:")
    print("  python src/train.py --experiment xor")
```

**Cell 5: Strengths and Weaknesses Analysis**

```python
# 4. Strengths and Weaknesses Analysis

print("Model Strengths and Weaknesses Analysis")
print("=" * 50)

# Analyze based on experiment results
strengths = []
weaknesses = []
observations = []

# Strength analysis
print("üü¢ STRENGTHS:")
if EXPERIMENT_NAME == 'xor':
    if 'correct' in locals() and correct >= 3:  # From previous cell
        strengths.append("Successfully learns non-linear XOR function")
        strengths.append("Demonstrates power of hidden layers with non-linear activations")
    
    if 'df' in locals() and not df.empty:
        final_loss = df['val_loss'].iloc[-1]
        if final_loss < 0.1:
            strengths.append(f"Achieves low validation loss ({final_loss:.4f})")
        
        if len(df) < 100:
            strengths.append("Converges quickly (few epochs needed)")

elif EXPERIMENT_NAME == 'iris_multiclass':
    strengths.append("Handles multi-class classification well")
    strengths.append("Works on real-world dataset")
    
elif EXPERIMENT_NAME == 'mnist_multiclass':
    strengths.append("Scales to high-dimensional input (784 features)")
    strengths.append("Handles complex multi-class problem (10 classes)")

# Weakness analysis
print("\nüî¥ WEAKNESSES:")
if EXPERIMENT_NAME == 'cifar10':
    weaknesses.append("Poor performance on complex image data")
    weaknesses.append("Lacks spatial awareness for images")
    weaknesses.append("Treats pixels as independent features")

# General weaknesses
weaknesses.extend([
    "Requires manual feature engineering",
    "No built-in translation invariance",
    "Fully connected layers are parameter-heavy",
    "Vanishing gradients in very deep networks"
])

# Print analysis
for i, strength in enumerate(strengths, 1):
    print(f"  {i}. {strength}")

if not strengths:
    print("  Analysis pending - need training results")

print("\nüî¥ WEAKNESSES:")
for i, weakness in enumerate(weaknesses, 1):
    print(f"  {i}. {weakness}")

# Observations
print("\nüîç OBSERVATIONS:")
observations.extend([
    "MLP is the foundation of deep learning",
    "Hidden layers enable learning complex functions",
    "Non-linear activations are crucial for expressiveness",
    "Backpropagation enables efficient training"
])

for i, observation in enumerate(observations, 1):
    print(f"  {i}. {observation}")
```

**Cell 6: Next Steps and Conclusions**

```python
# 5. Next Steps and Conclusions

print("Conclusions and Next Steps")
print("=" * 40)

# Summary of findings
print("üìã SUMMARY OF FINDINGS:")
print()

# Model performance summary
if 'accuracy' in locals():
    print(f"‚úì Model Accuracy: {accuracy:.1%}")
else:
    print("‚Ä¢ Model performance metrics: [Run training first]")

if 'df' in locals() and not df.empty:
    print(f"‚úì Training Convergence: {'Yes' if df['val_loss'].iloc[-5:].std() < 0.01 else 'No'}")
    print(f"‚úì Final Loss: {df['val_loss'].iloc[-1]:.6f}")
else:
    print("‚Ä¢ Training metrics: [Run training first]")

print(f"‚úì Visualizations Generated: {len(analyzed_plots) if 'analyzed_plots' in locals() else 0}")

# Key insights
print("\nüîë KEY INSIGHTS:")
insights = [
    "MLPs can learn non-linear functions through hidden layers",
    "Non-linear activations (ReLU, sigmoid) are essential",
    "Backpropagation enables efficient gradient computation",
    "Model capacity must match problem complexity"
]

for i, insight in enumerate(insights, 1):
    print(f"  {i}. {insight}")

# Limitations that motivate next model
print("\n‚ö†Ô∏è  LIMITATIONS (Motivating Next Model):")
limitations = [
    "No spatial awareness for image data",
    "Treats all input features as independent",
    "Requires flattening of spatial data",
    "Parameter count grows quickly with input size"
]

for i, limitation in enumerate(limitations, 1):
    print(f"  {i}. {limitation}")

# Next model motivation
print("\nüöÄ NEXT MODEL MOTIVATION:")
print("Based on the limitations observed, the next logical step is:")
print("‚Ä¢ Convolutional Neural Networks (CNNs)")
print("‚Ä¢ Key innovation: Spatial awareness through convolution")
print("‚Ä¢ Benefits: Translation invariance, parameter sharing")
print("‚Ä¢ Addresses: Poor performance on image data")

# Recommendations
print("\nüí° RECOMMENDATIONS:")
recommendations = [
    "Use MLPs for tabular data and feature learning",
    "Avoid MLPs for raw image data - use CNNs instead",
    "Consider regularization (dropout, weight decay) for complex problems",
    "Monitor for overfitting with validation curves"
]

for i, recommendation in enumerate(recommendations, 1):
    print(f"  {i}. {recommendation}")

print("\n" + "="*40)
print("Analysis Complete!")
print("="*40)
```

---

## **4. Notebook Quality Standards**

### **Code Quality Requirements**

* **Reproducibility**: All cells must run in order without errors
* **Clear Documentation**: Each cell has markdown explanation
* **Consistent Style**: Follow PEP 8 and project coding standards
* **Error Handling**: Graceful handling of missing files or failed operations

### **Educational Value Requirements**

* **Progressive Complexity**: Build understanding step by step
* **Visual Learning**: Include plots, diagrams, and visualizations
* **Interactive Elements**: Allow experimentation and exploration
* **Clear Explanations**: Connect theory to implementation

### **Technical Requirements**

* **Efficient Code**: Avoid unnecessary computations
* **Memory Management**: Clear variables when no longer needed
* **Cross-Platform**: Work on Windows, Mac, and Linux
* **Version Control**: Notebooks must be git-friendly (clear outputs before commit)

---

## **5. Validation Guidelines**

### **Pre-Commit Validation Checklist**

```python
# Notebook validation script
def validate_notebook(notebook_path):
    """Validate notebook meets quality standards."""
    checks = {
        'cells_run_successfully': False,
        'has_markdown_explanations': False,
        'imports_work': False,
        'outputs_cleared': False,
        'follows_template': False
    }
    
    # Implementation would check:
    # 1. All cells execute without errors
    # 2. Markdown cells provide explanations
    # 3. Required imports are present
    # 4. Outputs are cleared for git
    # 5. Follows expected template structure
    
    return checks
```

### **Content Validation Standards**

* **Theory Notebooks**: Mathematical accuracy, clear explanations
* **Code Notebooks**: Working imports, successful execution
* **Analysis Notebooks**: Results interpretation, conclusions drawn

### **Automated Validation**

```powershell
# Notebook validation script
function Test-Notebooks {
    param([string]$ModelPath)
    
    $notebooks = Get-ChildItem -Path "$ModelPath\notebooks" -Filter "*.ipynb"
    
    foreach ($notebook in $notebooks) {
        Write-Host "Validating $($notebook.Name)..."
        
        # Check if notebook runs
        jupyter nbconvert --to notebook --execute $notebook.FullName
        
        if ($LASTEXITCODE -eq 0) {
            Write-Host "‚úì $($notebook.Name) executes successfully"
        } else {
            Write-Host "‚úó $($notebook.Name) has execution errors"
        }
    }
}
```

---

## **6. Best Practices Summary**

### **Notebook Development Workflow**

1. **Start with Theory**: Begin with 01_Theory_and_Intuition.ipynb
2. **Implement Model**: Create working model in src/
3. **Document Code**: Complete 02_Code_Walkthrough.ipynb
4. **Run Experiments**: Execute training with --visualize
5. **Analyze Results**: Complete 03_Empirical_Analysis.ipynb
6. **Validate Quality**: Run validation checks
7. **Clear Outputs**: Clear outputs before committing

### **Common Pitfalls to Avoid**

* **Long Code Cells**: Keep cells focused and concise
* **Missing Explanations**: Every code cell needs markdown context
* **Hardcoded Paths**: Use relative paths and path validation
* **Missing Error Handling**: Check for file existence before loading
* **Unclear Conclusions**: Always summarize key findings

### **Maintenance Guidelines**

* **Update Regularly**: Keep notebooks current with code changes
* **Test Periodically**: Run validation checks monthly
* **Document Changes**: Update notebooks when model implementation changes
* **Archive Old Versions**: Keep historical versions for reference

---

## **7. "Conceptual" Models**

For models designated as "Conceptual" in your project charter, only the 01_Theory_and_Intuition.ipynb notebook needs to be created. This captures the essential learning without the overhead of a full implementation and analysis.

### **Conceptual Model Template**

Follow the Theory and Intuition template but add:

* **Comparison Section**: How does this compare to implemented models?
* **Implementation Notes**: What would be needed for full implementation?
* **Modern Relevance**: How does this relate to current approaches?

---

This comprehensive notebook strategy ensures that each model implementation includes rich, educational documentation that serves both learning and reference purposes while maintaining high quality standards throughout the project.