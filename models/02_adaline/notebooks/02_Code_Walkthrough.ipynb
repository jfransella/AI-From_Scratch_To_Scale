{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# **ADALINE: Code Walkthrough**\n",
        "## *From Theory to Implementation - Understanding the Delta Rule in Practice*\n",
        "\n",
        "---\n",
        "\n",
        "### **üìö Learning Objectives**\n",
        "\n",
        "By the end of this notebook, you will understand:\n",
        "\n",
        "1. **Implementation Details**: How ADALINE is coded in PyTorch\n",
        "2. **Delta Rule in Practice**: Seeing the algorithm step-by-step\n",
        "3. **Training Process**: Understanding the complete learning workflow\n",
        "4. **Comparison Code**: How our implementation differs from Perceptron\n",
        "5. **Educational Insights**: What the code teaches us about continuous learning\n",
        "\n",
        "---\n",
        "\n",
        "### **üéØ Overview**\n",
        "\n",
        "This notebook walks through our ADALINE implementation, explaining each component and how it implements the Delta Rule. We'll see how theory translates into working code.\n",
        "\n",
        "**What We'll Cover**:\n",
        "- Model architecture and initialization\n",
        "- The Delta Rule training loop\n",
        "- Comparison with Perceptron implementation\n",
        "- Key design decisions and their implications\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's start by examining our ADALINE implementation\n",
        "# First, let's look at the model architecture\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Add the src directory to our path\n",
        "sys.path.append('../src')\n",
        "\n",
        "# Import our ADALINE implementation\n",
        "from model import ADALINE\n",
        "from config import ADALINEConfig\n",
        "\n",
        "# Create a simple configuration for demonstration\n",
        "config = ADALINEConfig(\n",
        "    name=\"code_demo\",\n",
        "    description=\"Code walkthrough demonstration\",\n",
        "    input_size=2,\n",
        "    output_size=1,\n",
        "    learning_rate=0.01,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "print(\"üèóÔ∏è ADALINE Configuration:\")\n",
        "print(f\"  - Input size: {config.input_size}\")\n",
        "print(f\"  - Output size: {config.output_size}\")  \n",
        "print(f\"  - Learning rate: {config.learning_rate}\")\n",
        "print(f\"  - Architecture: Single linear layer (no activation)\")\n",
        "print(f\"  - Learning rule: Delta Rule (continuous updates)\")\n",
        "\n",
        "# Create the model\n",
        "model = ADALINE(config)\n",
        "print(f\"\\nüìä Model Information:\")\n",
        "model_info = model.get_model_info()\n",
        "for key, value in model_info.items():\n",
        "    if key != 'config':\n",
        "        print(f\"  - {key}: {value}\")\n",
        "\n",
        "print(f\"\\nüî¢ Model Parameters:\")\n",
        "print(f\"  - Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
        "print(f\"  - Weights: {model.linear.weight.shape}\")\n",
        "print(f\"  - Bias: {model.linear.bias.shape}\")\n",
        "print(f\"  - Initial weights: {model.linear.weight.data}\")\n",
        "print(f\"  - Initial bias: {model.linear.bias.data}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## **üèóÔ∏è Architecture Analysis**\n",
        "\n",
        "### **Key Design Decisions**\n",
        "\n",
        "Our ADALINE implementation makes several important design choices:\n",
        "\n",
        "#### **1. Linear Layer Only**\n",
        "```python\n",
        "self.linear = nn.Linear(config.input_size, config.output_size, bias=True)\n",
        "```\n",
        "- **No activation function** - this is crucial for ADALINE\n",
        "- Uses PyTorch's Linear layer for efficiency\n",
        "- Includes bias term (essential for learning)\n",
        "\n",
        "#### **2. Small Random Weight Initialization**\n",
        "```python\n",
        "def _initialize_weights(self):\n",
        "    with torch.no_grad():\n",
        "        self.linear.weight.normal_(0, 0.1)  # Small random weights\n",
        "        self.linear.bias.zero_()            # Zero bias\n",
        "```\n",
        "- **Why small weights?** Large weights can cause unstable learning\n",
        "- **Why zero bias?** Let the algorithm learn the appropriate bias\n",
        "\n",
        "#### **3. Training History Tracking**\n",
        "```python\n",
        "self.training_history = {\n",
        "    \"loss\": [],      # MSE at each epoch\n",
        "    \"mse\": [],       # Same as loss (for clarity)\n",
        "    \"epochs_trained\": 0\n",
        "}\n",
        "```\n",
        "- Essential for visualizing learning progress\n",
        "- Enables comparison with other algorithms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's examine the Delta Rule implementation step by step\n",
        "print(\"üîç DELTA RULE IMPLEMENTATION WALKTHROUGH\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create some sample data to trace through the algorithm\n",
        "import torch\n",
        "torch.manual_seed(42)  # For reproducible results\n",
        "\n",
        "# Simple 2D data: 4 points\n",
        "x_data = torch.tensor([\n",
        "    [1.0, 1.0],   # Point 1\n",
        "    [1.0, -1.0],  # Point 2  \n",
        "    [-1.0, 1.0],  # Point 3\n",
        "    [-1.0, -1.0]  # Point 4\n",
        "], dtype=torch.float32)\n",
        "\n",
        "# Target: AND logic (only positive when both inputs are positive)\n",
        "y_target = torch.tensor([\n",
        "    [1.0],   # 1 AND 1 = 1\n",
        "    [0.0],   # 1 AND -1 = 0\n",
        "    [0.0],   # -1 AND 1 = 0\n",
        "    [0.0]    # -1 AND -1 = 0\n",
        "], dtype=torch.float32)\n",
        "\n",
        "print(\"üìä Training Data:\")\n",
        "for i in range(len(x_data)):\n",
        "    print(f\"  x{i+1}: {x_data[i].numpy()} ‚Üí target: {y_target[i].item()}\")\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è Initial Model State:\")\n",
        "print(f\"  - Weights: {model.linear.weight.data.numpy()}\")\n",
        "print(f\"  - Bias: {model.linear.bias.data.item():.4f}\")\n",
        "\n",
        "# Let's trace through one training step manually\n",
        "print(f\"\\nüîÑ MANUAL DELTA RULE STEP:\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Take the first data point\n",
        "x_sample = x_data[0:1]  # Keep batch dimension\n",
        "y_sample = y_target[0:1]\n",
        "\n",
        "print(f\"Input: {x_sample.numpy().flatten()}\")\n",
        "print(f\"Target: {y_sample.item()}\")\n",
        "\n",
        "# Forward pass\n",
        "linear_output = model.forward(x_sample)\n",
        "print(f\"Linear output: {linear_output.item():.4f}\")\n",
        "\n",
        "# Calculate error\n",
        "error = y_sample - linear_output\n",
        "print(f\"Error: {error.item():.4f}\")\n",
        "\n",
        "# Show the Delta Rule calculation (manually)\n",
        "learning_rate = model.config.learning_rate\n",
        "weight_delta = learning_rate * error * x_sample\n",
        "bias_delta = learning_rate * error\n",
        "\n",
        "print(f\"\\nDelta Rule Calculations:\")\n",
        "print(f\"  - Learning rate: {learning_rate}\")\n",
        "print(f\"  - Weight delta: Œ∑ √ó error √ó input = {learning_rate} √ó {error.item():.4f} √ó {x_sample.numpy().flatten()}\")\n",
        "print(f\"    = {weight_delta.numpy().flatten()}\")\n",
        "print(f\"  - Bias delta: Œ∑ √ó error = {learning_rate} √ó {error.item():.4f} = {bias_delta.item():.4f}\")\n",
        "\n",
        "print(f\"\\nüìà This demonstrates the CONTINUOUS nature of ADALINE:\")\n",
        "print(f\"  - Error magnitude: {abs(error.item()):.4f} (not just 0 or 1)\")\n",
        "print(f\"  - Proportional update: Larger errors ‚Üí Larger weight changes\")\n",
        "print(f\"  - Every sample updates weights (even if prediction is 'close')\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
