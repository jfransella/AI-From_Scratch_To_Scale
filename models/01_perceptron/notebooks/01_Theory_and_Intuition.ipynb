{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# The Perceptron: Theory and Intuition üß†\n",
    "\n",
    "**The Birth of Artificial Neural Networks (1957)**\n",
    "\n",
    "> *\"The Perceptron represents the first step toward machines that can perceive, recognize, and make decisions based on what they 'see'.\"* - Frank Rosenblatt\n",
    "\n",
    "Welcome to the foundation of neural networks! This notebook explores the theoretical underpinnings and intuitive understanding of the Perceptron - the first artificial neural network capable of learning.\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Table of Contents\n",
    "\n",
    "1. [**Historical Context: The 5 Ws**](#historical)\n",
    "2. [**Mathematical Foundation**](#math)\n",
    "3. [**Architectural Intuition**](#architecture)  \n",
    "4. [**The Learning Process**](#learning)\n",
    "5. [**Capabilities and Limitations**](#limitations)\n",
    "6. [**Connection to Modern AI**](#modern)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a id=\"historical\"></a>\n",
    "## üï∞Ô∏è Historical Context: The 5 Ws\n",
    "\n",
    "### **WHO**: Frank Rosenblatt (1928-1971)\n",
    "- **Background**: American psychologist and computer scientist at Cornell University\n",
    "- **Vision**: Create machines that could mimic biological neural networks\n",
    "- **Inspiration**: Warren McCulloch and Walter Pitts' 1943 paper on artificial neurons\n",
    "\n",
    "### **WHAT**: The Perceptron Algorithm\n",
    "- **Definition**: A linear classifier that learns to separate data into two categories\n",
    "- **Innovation**: First artificial neural network capable of **learning from experience**\n",
    "- **Core Idea**: Adjust connection weights based on prediction errors\n",
    "\n",
    "### **WHEN**: 1957-1962\n",
    "- **1957**: Initial conception and mathematical formulation\n",
    "- **1958**: Publication in *Psychological Review*\n",
    "- **1962**: Physical implementation - the \"Mark I Perceptron\" \n",
    "- **1969**: Minsky & Papert's critique leads to \"AI Winter\"\n",
    "\n",
    "### **WHERE**: Cornell Aeronautical Laboratory\n",
    "- **Setting**: Post-WWII optimism about technology\n",
    "- **Context**: Early computers, cybernetics movement\n",
    "- **Goal**: Bridge psychology, biology, and engineering\n",
    "\n",
    "### **WHY**: The Dream of Thinking Machines\n",
    "- **Motivation**: Understand how the brain learns and recognizes patterns\n",
    "- **Promise**: Machines that could see, learn, and make decisions\n",
    "- **Impact**: Launched the field of machine learning\n",
    "\n",
    "---\n",
    "\n",
    "### üì∞ Historical Headlines\n",
    "\n",
    "**1958 New York Times**: *\"New Navy Device Learns By Doing\"*\n",
    "> \"The Navy revealed today a electronic computer that it expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.\"\n",
    "\n",
    "**The Reality**: A significant step toward AI, but the hype exceeded the capabilities!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a id=\"math\"></a>\n",
    "## üìê Mathematical Foundation\n",
    "\n",
    "### **The Perceptron Equation**\n",
    "\n",
    "The Perceptron computes a simple weighted sum of inputs:\n",
    "\n",
    "$$y = f(\\mathbf{w} \\cdot \\mathbf{x} + b)$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{x} = [x_1, x_2, ..., x_d]$ is the input vector\n",
    "- $\\mathbf{w} = [w_1, w_2, ..., w_d]$ are the weights  \n",
    "- $b$ is the bias term\n",
    "- $f()$ is the activation function (step function)\n",
    "\n",
    "### **Step Activation Function**\n",
    "\n",
    "$$f(z) = \\begin{cases} \n",
    "1 & \\text{if } z \\geq 0 \\\\\n",
    "0 & \\text{if } z < 0 \n",
    "\\end{cases}$$\n",
    "\n",
    "This creates a **linear decision boundary** in the input space.\n",
    "\n",
    "### **The Learning Rule**\n",
    "\n",
    "When the Perceptron makes an error, it updates its weights:\n",
    "\n",
    "$$\\mathbf{w}_{new} = \\mathbf{w}_{old} + \\eta (t - y) \\mathbf{x}$$\n",
    "$$b_{new} = b_{old} + \\eta (t - y)$$\n",
    "\n",
    "Where:\n",
    "- $\\eta$ is the learning rate\n",
    "- $t$ is the target (correct) output  \n",
    "- $y$ is the predicted output\n",
    "- $(t - y)$ is the error\n",
    "\n",
    "### **Geometric Interpretation**\n",
    "\n",
    "The decision boundary is a **hyperplane** defined by:\n",
    "$$\\mathbf{w} \\cdot \\mathbf{x} + b = 0$$\n",
    "\n",
    "- **Above the line**: Positive class (output = 1)\n",
    "- **Below the line**: Negative class (output = 0)\n",
    "- **Learning**: Rotates and shifts this line to separate the classes\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Perceptron Math Demonstration\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simple 2D example\n",
    "print(\"üî¢ Perceptron Math Demonstration\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Example input\n",
    "x = np.array([0.5, 1.2])  # 2D input point\n",
    "w = np.array([1.0, -0.5])  # weights\n",
    "b = 0.3  # bias\n",
    "\n",
    "# Compute weighted sum\n",
    "z = np.dot(w, x) + b\n",
    "print(f\"Input x: {x}\")\n",
    "print(f\"Weights w: {w}\")  \n",
    "print(f\"Bias b: {b}\")\n",
    "print(f\"Weighted sum z = w¬∑x + b: {z:.3f}\")\n",
    "\n",
    "# Apply step function\n",
    "output = 1 if z >= 0 else 0\n",
    "print(f\"Step function output: {output}\")\n",
    "\n",
    "# Show what learning looks like\n",
    "print(\"\\nüéØ Learning Example:\")\n",
    "print(\"-\" * 20)\n",
    "target = 0  # correct answer should be 0\n",
    "prediction = output  # we predicted 1\n",
    "error = target - prediction\n",
    "learning_rate = 0.1\n",
    "\n",
    "print(f\"Target: {target}\")\n",
    "print(f\"Prediction: {prediction}\")\n",
    "print(f\"Error: {error}\")\n",
    "\n",
    "if error != 0:\n",
    "    # Update weights\n",
    "    w_new = w + learning_rate * error * x\n",
    "    b_new = b + learning_rate * error\n",
    "    \n",
    "    print(f\"Weight update: w = {w} + {learning_rate} √ó {error} √ó {x} = {w_new}\")\n",
    "    print(f\"Bias update: b = {b} + {learning_rate} √ó {error} = {b_new}\")\n",
    "else:\n",
    "    print(\"No error - no weight update needed!\")\n",
    "\n",
    "print(\"\\nüí° Key Insight: The Perceptron only learns from mistakes!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a id=\"architecture\"></a>\n",
    "## üèóÔ∏è Architectural Intuition\n",
    "\n",
    "### **Biological Inspiration**\n",
    "\n",
    "The Perceptron was inspired by biological neurons:\n",
    "\n",
    "```\n",
    "BIOLOGICAL NEURON          ARTIFICIAL PERCEPTRON\n",
    "      \n",
    "Dendrites  ‚îÄ‚îê                Inputs  ‚îÄ‚îê\n",
    "Dendrites  ‚îÄ‚î§                Inputs  ‚îÄ‚î§\n",
    "Dendrites  ‚îÄ‚î§‚îÄ‚ñ∫ [Cell Body] ‚îÄ‚ñ∫ Inputs  ‚îÄ‚î§‚îÄ‚ñ∫ [Œ£ + f] ‚îÄ‚ñ∫ Output\n",
    "Dendrites  ‚îÄ‚î§                Inputs  ‚îÄ‚î§\n",
    "Dendrites  ‚îÄ‚îò                Inputs  ‚îÄ‚îò\n",
    "\n",
    "- Dendrites = Input features\n",
    "- Synapses = Weights  \n",
    "- Cell body = Summation + activation\n",
    "- Axon = Output\n",
    "```\n",
    "\n",
    "### **Information Flow**\n",
    "\n",
    "1. **Input Stage**: Receive feature values (x‚ÇÅ, x‚ÇÇ, ..., x‚Çô)\n",
    "2. **Weighting**: Multiply each input by its weight (w‚ÇÅx‚ÇÅ, w‚ÇÇx‚ÇÇ, ..., w‚Çôx‚Çô)  \n",
    "3. **Summation**: Add all weighted inputs plus bias (Œ£w·µ¢x·µ¢ + b)\n",
    "4. **Activation**: Apply step function to get binary output\n",
    "5. **Decision**: Output represents class prediction (0 or 1)\n",
    "\n",
    "### **Key Architectural Properties**\n",
    "\n",
    "- **Single Layer**: Only one processing unit (no hidden layers)\n",
    "- **Linear**: Decision boundary is always a straight line/hyperplane\n",
    "- **Binary**: Can only distinguish between two classes\n",
    "- **Feedforward**: Information flows only in one direction\n",
    "- **Trainable**: Weights adapt based on experience\n",
    "\n",
    "### **Conceptual Metaphor: The Digital Bouncer**\n",
    "\n",
    "Think of the Perceptron as a digital bouncer at a club:\n",
    "\n",
    "- **Inputs**: Person's attributes (age, dress code, ID, etc.)\n",
    "- **Weights**: How much each attribute matters for admission\n",
    "- **Bias**: General strictness/leniency of the club\n",
    "- **Decision**: Let in (1) or deny entry (0)\n",
    "- **Learning**: Adjust criteria based on mistakes\n",
    "\n",
    "If the bouncer makes a wrong decision, they adjust their criteria for next time!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Decision Boundary Concept\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Create sample 2D data\n",
    "np.random.seed(42)\n",
    "class_0 = np.random.randn(20, 2) + [-1, -1]  # Bottom-left cluster  \n",
    "class_1 = np.random.randn(20, 2) + [1, 1]    # Top-right cluster\n",
    "\n",
    "# Plot 1: Linearly Separable Data (Perceptron can solve)\n",
    "ax1.scatter(class_0[:, 0], class_0[:, 1], c='red', marker='o', label='Class 0', alpha=0.7)\n",
    "ax1.scatter(class_1[:, 0], class_1[:, 1], c='blue', marker='s', label='Class 1', alpha=0.7)\n",
    "\n",
    "# Draw a decision boundary\n",
    "x_line = np.linspace(-4, 4, 100)\n",
    "y_line = -x_line + 0.5  # Simple linear boundary\n",
    "ax1.plot(x_line, y_line, 'k--', linewidth=2, label='Decision Boundary')\n",
    "ax1.fill_between(x_line, y_line, 4, alpha=0.1, color='blue', label='Class 1 Region')\n",
    "ax1.fill_between(x_line, -4, y_line, alpha=0.1, color='red', label='Class 0 Region')\n",
    "\n",
    "ax1.set_title('Linearly Separable\\n(Perceptron CAN solve)', fontsize=12, weight='bold')\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(-4, 4)\n",
    "ax1.set_ylim(-4, 4)\n",
    "\n",
    "# Plot 2: XOR-like Data (Perceptron cannot solve)\n",
    "xor_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "xor_labels = np.array([0, 1, 1, 0])  # XOR pattern\n",
    "\n",
    "colors = ['red' if label == 0 else 'blue' for label in xor_labels]\n",
    "markers = ['o' if label == 0 else 's' for label in xor_labels]\n",
    "\n",
    "for i, (point, color, marker) in enumerate(zip(xor_data, colors, markers)):\n",
    "    ax2.scatter(point[0], point[1], c=color, marker=marker, s=100, \n",
    "                label=f'Class {xor_labels[i]}' if i < 2 else \"\")\n",
    "\n",
    "# Try to draw ANY single line - it cannot separate XOR\n",
    "ax2.plot([-0.2, 1.2], [0.5, 0.5], 'k--', linewidth=2, alpha=0.5, label='Any single line fails')\n",
    "ax2.plot([0.5, 0.5], [-0.2, 1.2], 'k--', linewidth=2, alpha=0.5)\n",
    "\n",
    "ax2.set_title('XOR Problem\\n(Perceptron CANNOT solve)', fontsize=12, weight='bold')\n",
    "ax2.set_xlabel('Feature 1')\n",
    "ax2.set_ylabel('Feature 2')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(-0.2, 1.2)\n",
    "ax2.set_ylim(-0.2, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéØ Key Insight: The Perceptron can only learn patterns that are linearly separable!\")\n",
    "print(\"   This fundamental limitation led to the 'AI Winter' and motivated multi-layer networks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a id=\"learning\"></a>\n",
    "## üéì The Learning Process\n",
    "\n",
    "### **Perceptron Learning Algorithm**\n",
    "\n",
    "The Perceptron learns through **supervised learning** with these steps:\n",
    "\n",
    "1. **Initialize**: Start with random weights and zero bias\n",
    "2. **Present Sample**: Show the Perceptron an input-output pair\n",
    "3. **Make Prediction**: Compute output using current weights  \n",
    "4. **Check Error**: Compare prediction with correct answer\n",
    "5. **Update Weights**: If wrong, adjust weights to reduce error\n",
    "6. **Repeat**: Continue until no more errors (or max iterations)\n",
    "\n",
    "### **Learning Rule Intuition**\n",
    "\n",
    "The weight update rule has beautiful geometric interpretation:\n",
    "\n",
    "```\n",
    "w_new = w_old + Œ∑(target - prediction) √ó input\n",
    "```\n",
    "\n",
    "**What this means:**\n",
    "- **Correct prediction**: No change (target - prediction = 0)\n",
    "- **False positive**: Reduce weights (target=0, prediction=1, so multiply by -1)  \n",
    "- **False negative**: Increase weights (target=1, prediction=0, so multiply by +1)\n",
    "- **Learning rate Œ∑**: Controls how big steps to take\n",
    "\n",
    "### **Convergence Guarantee** üéØ\n",
    "\n",
    "**Perceptron Convergence Theorem**: \n",
    "If the data is linearly separable, the Perceptron is **guaranteed** to find a solution in finite time!\n",
    "\n",
    "This was revolutionary - a provable learning algorithm!\n",
    "\n",
    "### **Why It Works**\n",
    "\n",
    "- Each mistake moves the decision boundary closer to the correct solution\n",
    "- The algorithm cannot get stuck in local optima (there's only one optimum)\n",
    "- Mathematical proof shows finite convergence for separable data\n",
    "\n",
    "### **Learning Rate Effects**\n",
    "\n",
    "- **Too small**: Slow learning, many iterations\n",
    "- **Too large**: Might overshoot, unstable learning  \n",
    "- **Just right**: Fast, stable convergence\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a id=\"limitations\"></a>\n",
    "## ‚ö†Ô∏è Capabilities and Limitations\n",
    "\n",
    "### **What the Perceptron CAN Do** ‚úÖ\n",
    "\n",
    "- **Linear Classification**: Perfect for linearly separable data\n",
    "- **Boolean Functions**: AND, OR, NAND gates\n",
    "- **Simple Patterns**: Basic pattern recognition tasks\n",
    "- **Guaranteed Learning**: Provable convergence for valid problems\n",
    "- **Fast Training**: Simple, efficient algorithm\n",
    "- **Interpretable**: Weights show feature importance\n",
    "\n",
    "### **What the Perceptron CANNOT Do** ‚ùå\n",
    "\n",
    "- **XOR Function**: The famous limitation that caused the AI Winter\n",
    "- **Non-linear Patterns**: Curved boundaries, complex shapes\n",
    "- **Multi-class**: Only handles binary classification natively  \n",
    "- **Complex Features**: Cannot learn feature interactions\n",
    "- **Non-separable Data**: Gets stuck on overlapping distributions\n",
    "\n",
    "### **The XOR Crisis (1969)** üí•\n",
    "\n",
    "Minsky and Papert proved that a single Perceptron cannot solve XOR:\n",
    "\n",
    "| Input A | Input B | XOR Output |\n",
    "|---------|---------|------------|\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1 |  \n",
    "| 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 |\n",
    "\n",
    "**Why XOR is impossible**: No single straight line can separate the 1s from the 0s!\n",
    "\n",
    "This revelation led to:\n",
    "- Funding cuts for neural network research\n",
    "- The \"AI Winter\" of the 1970s-80s\n",
    "- Focus shift to symbolic AI and expert systems\n",
    "\n",
    "### **Historical Impact**\n",
    "\n",
    "**The Good**:\n",
    "- Launched machine learning as a field\n",
    "- Introduced the concept of learning algorithms  \n",
    "- Inspired biological neural modeling\n",
    "\n",
    "**The Bad**:\n",
    "- Overhyped promises led to disappointment\n",
    "- Limitations weren't initially understood\n",
    "- Caused decades-long setback in neural network research\n",
    "\n",
    "**The Redemption**:\n",
    "- Multi-layer networks (1980s) solved the XOR problem\n",
    "- Backpropagation enabled deep learning\n",
    "- Modern AI traces back to these foundations\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a id=\"modern\"></a>\n",
    "## üöÄ Connection to Modern AI\n",
    "\n",
    "### **The Perceptron's DNA in Modern Neural Networks**\n",
    "\n",
    "Every modern neural network contains Perceptron-like units:\n",
    "\n",
    "```\n",
    "PERCEPTRON (1957)          MODERN NEURON (2024)\n",
    "     \n",
    "[Inputs] ‚Üí [Œ£ + f] ‚Üí [Output]    [Inputs] ‚Üí [Œ£ + f] ‚Üí [Output]\n",
    "\n",
    "Same basic structure! The evolution:\n",
    "- Multiple layers instead of one\n",
    "- Better activation functions (ReLU, sigmoid)\n",
    "- Advanced optimization (Adam, SGD with momentum)\n",
    "- Regularization techniques (dropout, batch norm)\n",
    "```\n",
    "\n",
    "### **Modern Applications**\n",
    "\n",
    "**Large Language Models (GPT, ChatGPT)**:\n",
    "- Billions of Perceptron-like units working together\n",
    "- Same basic weight update principles\n",
    "- Linear transformations everywhere\n",
    "\n",
    "**Computer Vision (CNNs)**:\n",
    "- Convolutional layers = spatially-connected Perceptrons  \n",
    "- Each filter is a specialized Perceptron\n",
    "\n",
    "**Recommendation Systems**:\n",
    "- Linear models for user-item predictions\n",
    "- Logistic regression = Perceptron with sigmoid\n",
    "\n",
    "### **Key Insights for Modern AI**\n",
    "\n",
    "1. **Scalability**: What works for one unit scales to billions\n",
    "2. **Simplicity**: Complex behaviors emerge from simple rules\n",
    "3. **Learning**: Error-driven improvement is universal\n",
    "4. **Linearity**: Even \"non-linear\" networks use linear building blocks\n",
    "5. **Gradients**: All modern training uses Perceptron-like weight updates\n",
    "\n",
    "### **What We Learned for the Future**\n",
    "\n",
    "- **Single layers have limits** ‚Üí Multi-layer networks\n",
    "- **Linear boundaries aren't enough** ‚Üí Non-linear activations  \n",
    "- **Local learning is powerful** ‚Üí Backpropagation\n",
    "- **Simple algorithms scale** ‚Üí Deep learning revolution\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Takeaways\n",
    "\n",
    "**The Perceptron taught us that:**\n",
    "\n",
    "1. **Learning is possible** - Machines can improve from experience\n",
    "2. **Simple rules create complex behaviors** - Basic math ‚Üí intelligent decisions  \n",
    "3. **Understanding limitations drives innovation** - XOR problem ‚Üí MLPs\n",
    "4. **Biological inspiration works** - Neural metaphors are powerful\n",
    "5. **Foundation matters** - Every modern AI system builds on these principles\n",
    "\n",
    "**Next Steps**: Ready to see the code? Head to notebook 02 for implementation details!\n",
    "\n",
    "**Remember**: The Perceptron isn't just history - it's the foundation of every neural network you'll ever encounter. Master this, and you understand the core of all AI! üß†‚ú®\n",
    "\n",
    "---\n",
    "\n",
    "*\"The Perceptron was not just a model; it was a proof of concept that machines could learn. Everything that followed built upon this fundamental insight.\"* - Anonymous AI Historian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
